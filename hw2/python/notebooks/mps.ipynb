{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3886f0f-4a8c-48d0-aa4c-b9e24c6bc8ff",
   "metadata": {},
   "source": [
    "# Matrix product state representation for ground states\n",
    "\n",
    "$\n",
    "\\require{physics}\n",
    "\\def\\bm{\\boldsymbol}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c557d62-658a-47e2-96b1-2f10e2c1b27e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Clearly, God did not conceive the universe as a point because God would not be\n",
    "able to take the outer product of two points to obtain vectors.\n",
    "So in the beginning, there were at least two vectors, and God took the outer\n",
    "product to make higher order tensors, the inner product to make lower order \n",
    "tensors: these were the binary operators on tensors.\n",
    "God defined these tensors over a field and also used the transpose and inverse.\n",
    "We believe God may have saved us because God invented index notation \n",
    "so that any of this could make sense.\n",
    "Therefore tensors, their fields, their classes, their algebras, and their calculus\n",
    "is the only way that it is convenient to express the geometry of the universe.\n",
    "\n",
    "Sometimes smart people find ways to represent large tensors by smaller tensors,\n",
    "and this assignment is originated in this observation.\n",
    "The goal is to rewrite a tensor of dimension $d^L$ which describes the state\n",
    "of a qu$d$it chain of known $L$ength as a matrix product.\n",
    "We do so using the SVD and Schmidt decomposition techniques explored earlier.\n",
    "\n",
    "Other projects seem pretty useful to look into for helpful\n",
    "[resources](https://tensornetwork.org/) and\n",
    "[software](https://github.com/google/TensorNetwork).\n",
    "This is a [useful paper](https://arxiv.org/abs/1008.3477) cited in the\n",
    "assignment on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333a3f7-5e4e-44a8-b76f-75a9bcd3aab1",
   "metadata": {},
   "source": [
    "## MPS representation\n",
    "\n",
    "One asks how you actually do this task efficiently, which comes down to knowing\n",
    "how to represent the MPS wavefunction in memory.\n",
    "Choose a positive integer value for $d$ and $L$.\n",
    "The representation is of the form\n",
    "$$\n",
    "    \\psi_{\\sigma_1, \\dots, \\sigma_L} = \n",
    "        \\sum_{\\alpha_0, \\dots, \\alpha_L}\n",
    "        \\left( \\prod_{i=0}^{L-1} A_{\\alpha_i \\alpha_{i+1}}^{\\sigma_{i+1}} \\right)\n",
    "$$\n",
    "where\n",
    "\\begin{align}\n",
    "    \\dim(\\alpha_i) \\leq \n",
    "        \\begin{cases}\n",
    "            d^i\n",
    "                &\\qq{i \\leq L/2}\n",
    "            \\\\\\\\\n",
    "            d^{L-i}\n",
    "                &\\qq{i > L/2}\n",
    "        \\end{cases}\n",
    "    .\n",
    "\\end{align}\n",
    "The form of this dimensionality is due to the way the mps representation is \n",
    "constructed: a tensor is reshaped and has its SVD taken many times,\n",
    "each new time after another reshaping and SVD.\n",
    "Essentially these dimensions are the maximal ranks of a matrix as it is reshaped\n",
    "from $(1, d^L)$ to $(d^L, 1)$ when exchanging rows to columns $d$ at a time.\n",
    "Now an approximation scheme for MPS is a map:\n",
    "\\begin{align}\n",
    "    r(i) : \\{i\\} \\to \\{1, \\dots, \\max(1, \\min(d r(i-1), \\dim(\\alpha_i))\\}\n",
    "\\end{align}\n",
    "with $r(0) = 1$\n",
    "that specifies the number of rows retained at each bond index.\n",
    "This is a finite but large function space, and of the many approximation schemes\n",
    "it often makes sense to choose a simple one, such as\n",
    "$r(i) = \\min(\\chi, \\dim(\\alpha_i))$ for constant $\\chi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c31f1-d006-453c-b8f3-76279f0170d5",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "The scaling of storage requirements as a function of the approximation scheme\n",
    "can be calculated succinctly as $d \\sum_i r(i) r(i+1)$.\n",
    "Taking $r=\\dim(\\alpha_i)$ makes for an inefficient full representation of the\n",
    "because of the $d^L$ scaling for the individual matrices.\n",
    "For what truncation ranks $r$ is MPS an efficient storage scheme?\n",
    "We will test the accuracy of the scheme later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c580b9-6c50-4e44-aa50-c1fd0c2122f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Program\n",
    "\n",
    "- $d=2, r=\\chi=1$ ($r=1$ is a special case where the storage format can be optimized)\n",
    "- For $h \\in \\{ 1, 5/4 \\}$\n",
    "- Take the ground state in the open system at large L\n",
    "- Compute the MPS approximation of the wavefunction, varying the bond length $k$\n",
    "- Calculate the actual reduction in storage space (np.size)\n",
    "- Contract the indices of the tensors of MPS to obtain exponentially large\n",
    "$\\ket{\\tilde{\\psi}_{gs} (k)}$.\n",
    "- Compute the overlap $\\braket{\\tilde{\\psi}\\_{gs} (k)}{\\psi\\_{gs}}$\n",
    "\n",
    "### Efficient calculations with MPS\n",
    "\n",
    "- Use MPS to calculate\n",
    "$E(k) = \\ev{H}{\\tilde{\\psi}\\_{gs} (k)} / \\braket{\\tilde{\\psi}\\_{gs} (k)}$\n",
    "- Compute the same correlation functions as in Assignment 1 at both values of \n",
    "the order parameter and study the convergence in $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559015c0-0632-42d3-b2a5-bd0dfa5c3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ph121c_lxvm import basis, tfim, tests, tensor, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb648d-c5b0-4ad6-a326-6049b51d43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "measurements = {\n",
    "    'bc': [],\n",
    "    'L' : [],\n",
    "    'h' : [],\n",
    "    'k' : [],\n",
    "    'N' : [],\n",
    "    'nm': [],\n",
    "    'E' : [],\n",
    "    'Ek': [],\n",
    "    'ip': [],\n",
    "    'Cz': [],\n",
    "    'Mz': [],\n",
    "}\n",
    "\n",
    "for oper_params in tests.tfim_sweep(\n",
    "    L = [16],\n",
    "    h = [1, 1.2],\n",
    "    bc= ['o'],\n",
    "):\n",
    "    job = dict(\n",
    "        oper=tfim.z.H_sparse,\n",
    "        oper_params=oper_params,\n",
    "        solver=sla.eigsh,\n",
    "        solver_params={ \n",
    "            'k' : 6, \n",
    "            'which' : 'BE',\n",
    "        },\n",
    "    )\n",
    "    evals, evecs = data.jobs.obtain(**job)\n",
    "    \n",
    "    # construct local operators\n",
    "    sx = np.array([[0, 1], [1, 0]], dtype='float64')\n",
    "    sz = np.array([[1, 0], [0, -1]], dtype='float64')\n",
    "        \n",
    "    C = tensor.mpo(oper_params['L'], d=2)\n",
    "    C[0] = sz\n",
    "    C[oper_params['L'] // 2] = sz\n",
    "    \n",
    "    # Construct operators which are sums of local operators\n",
    "    # (sums of mpos are not always mpos)\n",
    "    H = []\n",
    "    M = []\n",
    "    \n",
    "    # Hamiltonian\n",
    "    ## z terms\n",
    "    for i in range(oper_params['L'] - 1 + (oper_params['bc'] == 'c')):\n",
    "        H.append(tensor.mpo(oper_params['L'], d=2))\n",
    "        H[-1][i] = -sz\n",
    "        H[-1][(i+1) % oper_params['L']] = sz\n",
    "    ## x terms\n",
    "    for i in range(oper_params['L']):\n",
    "        H.append(tensor.mpo(oper_params['L'], d=2))\n",
    "        H[-1][i] = -oper_params['h'] * sx\n",
    "        \n",
    "    # Magnetization\n",
    "    for i, j in product(np.arange(oper_params['L']), repeat=2):\n",
    "        M.append(tensor.mpo(oper_params['L'], d=2))\n",
    "        M[-1][i] = sz / (oper_params['L'] ** 2)\n",
    "        M[-1][j] = sz\n",
    "    \n",
    "    # Do the MPS\n",
    "    chi_max = 20\n",
    "    rank = tensor.bond_rank(chi_max, oper_params['L'], d=2)\n",
    "    A = tensor.mps(evecs[:, 0], , L=oper_params['L'], d=2)\n",
    "\n",
    "    for i in range(chi_max - 1):\n",
    "        rank = tensor.bond_rank(chi_max - i, oper_params['L'], d=2)\n",
    "        A.lower_rank(rank)\n",
    "        measurements['bc'].append(\n",
    "            oper_params['bc']\n",
    "        )\n",
    "        measurements['L'].append(\n",
    "            oper_params['L']\n",
    "        )\n",
    "        measurements['h'].append(\n",
    "            oper_params['h']\n",
    "        )\n",
    "        measurements['k'].append(\n",
    "            chi_max - i\n",
    "        )\n",
    "        measurements['N'].append(\n",
    "            A.size()\n",
    "        )\n",
    "        measurements['nm'].append(\n",
    "            A.inner(A)\n",
    "        )\n",
    "        measurements['E'].append(\n",
    "            evals[0]\n",
    "        )\n",
    "        measurements['ip'].append(\n",
    "            np.inner(A.v, A.contract_bonds()) / np.sqrt(measurements['nm'][-1])\n",
    "        )\n",
    "        measurements['Ek'].append(\n",
    "            sum(A.expval(e) for e in H) / measurements['nm'][-1]\n",
    "        )\n",
    "        measurements['Cz'].append(\n",
    "            A.expval(C) / measurements['nm'][-1]\n",
    "        )\n",
    "        measurements['Mz'].append(\n",
    "            sum(A.expval(e) for e in M) / measurements['nm'][-1]\n",
    "        )\n",
    "        \n",
    "df = pd.DataFrame(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fcd0a-5ba1-48c1-bb15-42898257305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture plot\n",
    "# make the k saturation plots\n",
    "myplots = ['Ek', 'N', 'ip', 'Cz', 'Mz']\n",
    "ncol = 2\n",
    "nrow = len(myplots) // 2 + len(myplots) % 2\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol)\n",
    "\n",
    "for i, row in enumerate(axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        if (ncol * i + j) < len(myplots):\n",
    "            for h in [1, 1.2]:\n",
    "                ax.plot(\n",
    "                    df[df.h == h]['k'].values,\n",
    "                    df[df.h == h][myplots[i * ncol + j]].values,\n",
    "                    label='h=' + str(h),\n",
    "                )\n",
    "            ax.set_xlabel('k')\n",
    "            ax.set_ylabel(myplots[i * ncol + j])\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "        else:\n",
    "            ax.set_axis_off()\n",
    "            ax.legend(handles, labels, loc='center')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0e58e-a381-4f2d-afe8-3c820d43ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture corr\n",
    "# Make the long-range correlation plots\n",
    "rank = tensor.bond_rank(chi_max, oper_params['L'], d=2)\n",
    "A = tensor.mps(evecs[:, 0], rank, L=oper_params['L'], d=2)\n",
    "\n",
    "C = np.empty(oper_params['L'], dtype='object')\n",
    "x = [0]\n",
    "y = [1]\n",
    "C[0] = sz\n",
    "for i in range(1, oper_params['L']):\n",
    "    C[i] = sz\n",
    "    x.append(i)\n",
    "    y.append(A.expval(C) / measurements['nm'][-1])\n",
    "    C[i] = None\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel('r')\n",
    "ax.set_ylabel('$C^{zz}_r$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18d98e-6040-4ebd-b81a-e7de881750da",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "I first wanted to show some of the numerical results in decimal form so that\n",
    "I have an opportunity to explain my abbreviations, and to show that the\n",
    "calculations are relatively accurate.\n",
    "The table below shows how several calculated quanties vary as a function of $k$,\n",
    "The quantities of interest are $N$, the actual number of coefficients stored in\n",
    "the mps approximation, $E_k$, the MPS expectation value of the energy of the \n",
    "ground state MPS wavefunction, $nm$, the norm of the wavefunction, $ip$, the\n",
    "overlap of the ground state vector with the MPS wavefunction after contracting\n",
    "its virtual indices, $C^{zz}$, the two-point spin correlation function measured\n",
    "at a distance of $L/2$, and $M^{zz}$, which is the normalized expected value\n",
    "of the magnetization of the chain.\n",
    "These values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96ef79-5f16-453b-80f0-7935c4f5eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2161122-ee33-4f1e-8c67-e3fb59d70b93",
   "metadata": {},
   "source": [
    "When $k=2$, the MPS values are correct to within 0.1% of the original, such\n",
    "as by comparing $E$, the exact diagonalization energy, with $E_k$, or the norm\n",
    "with the expected value of 1.\n",
    "However, the values appear to saturate as $k$ grows and already at $k=4$ it\n",
    "appears though some of the quantites are the same as $k=5$ with the 8 or so\n",
    "digits we can see.\n",
    "In particular, we see that the MPS ground state energy decreases towards the\n",
    "minimal exact value as the quality of approximation improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41fd3ef-79f7-43a8-bf7e-809313ccdd12",
   "metadata": {},
   "source": [
    "## Saturation of bond dimension\n",
    "\n",
    "We can visualize some of these values to get a wider look at the dependence on\n",
    "$k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3495f-58db-4b85-a39f-65ef8078b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca534d-2e81-4950-8210-a5c510e974ab",
   "metadata": {},
   "source": [
    "It appears that the values have all saturated long before reaching the maximum\n",
    "bond dimension of $k=20$ -- already it seems that at $k=5$ there won't be\n",
    "noticeable changes in any quantity, except for $N$ which is the rapidly-growing\n",
    "storage requirement (compare this to $2^{16} = 65,536$ coefficients in the\n",
    "dense vector representation of the wavefunction).\n",
    "Across two different values of $h$, at the critical value and in the paramagnetic\n",
    "phase (the paramagnetic effect is stronger for a larger system size such as \n",
    "this one, $L=16$), we observe the same physics as in the first assignment.\n",
    "The values of the correlation functions are two to three times higher for $h=1$\n",
    "than in the transverse-field dominated $h=1.2$ regime.\n",
    "In addition, due to this value of $h$, the ground state energy is lower for\n",
    "larger $h$, as observed last time, due to the influence of the aligning effect\n",
    "of the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e64ab0-7e79-4fc8-8e9d-42852382b0c4",
   "metadata": {},
   "source": [
    "### Verifying long-range correlations\n",
    "\n",
    "Another thing to verify is not only $C^{zz}$ at the half-way point in the chain,\n",
    "but correlation as a function of all distances at the chain.\n",
    "I repeated this measurement from the first assignment in the MPS representation,\n",
    "using the reference spin as the left boundary and measuring the $z$ spin\n",
    "correlation with the rest of the chain.\n",
    "The plot below, at $L=16$ and $h=1.2$ and open boundary conditions, shows how\n",
    "the correlations decay exponentially with the distance due to the strong effect\n",
    "of the transverse terms in the TFIM Hamiltonian in the paramagnetic phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c169a18-deb3-4829-a902-2a17f6385acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c973c-a69a-4f12-9bf2-37767d53f828",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We have implemented a functional MPS representation that allows for a massive\n",
    "reduction in storage space ($k=5$ corresponds to 620 coefficients versus \n",
    "65,536 in the dense wavefunction at $L=16$).\n",
    "This was possible because the Hamiltonian of this system is 2-local and\n",
    "with open boundary conditions, which curb the entanglement entropy of the system\n",
    "so that we can truncate the smallest Schmidt values to good approximation at\n",
    "each step in the MPS representation.\n",
    "In particular, this is true for very few states, those with an area law for the\n",
    "entanglement entropy, such as the ground state and most excited state.\n",
    "\n",
    "My implementation of mps is practical and has been tested for qudit systems\n",
    "for $d=3, 4$ in the `ph121c_lxvm.tests.tensor` module.\n",
    "Ultimately, if we consider translation invariant systems, we should test\n",
    "that the order in which we do MPS in the chain does not affect the results.\n",
    "I think with my current code on permutations in the `ph121c_lxvm.basis` module\n",
    "makes this possible by adding one line of code that I have commented in the\n",
    "`__init__` method for the `tensor.mps` class.\n",
    "However, that permutation code only works for $d=2$, so I'd have to put some\n",
    "thought into what algorithm can be used for larger $d$ because we lose the\n",
    "convenience of binary to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a69fe1-4e52-4042-b6d0-df7c159c7365",
   "metadata": {},
   "source": [
    "## Testing the ceiling\n",
    "\n",
    "I will do one last test to see the feasibility/runtime of the MPS scheme at\n",
    "$L=20$ with the bond dimension of $k=20$, while also calculating the expected\n",
    "energy of the state, at all truncations of $k$ from 1 to 5.\n",
    "The idea is to see whether the larger system size poses difficulties on the\n",
    "runtime or the accuracy of the approximation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb173b-36ed-46d9-9ab2-15de1b7832e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for oper_params in tests.tfim_sweep(\n",
    "    L = [20],\n",
    "    h = [1],\n",
    "    bc= ['o'],\n",
    "):\n",
    "    job = dict(\n",
    "        oper=tfim.z.H_sparse,\n",
    "        oper_params=oper_params,\n",
    "        solver=sla.eigsh,\n",
    "        solver_params={ \n",
    "            'k' : 6, \n",
    "            'which' : 'BE',\n",
    "        },\n",
    "    )\n",
    "    evals, evecs = data.jobs.obtain(**job)\n",
    "    \n",
    "    # construct local operators\n",
    "    sx = np.array([[0, 1], [1, 0]], dtype='float64')\n",
    "    sz = np.array([[1, 0], [0, -1]], dtype='float64')\n",
    "    \n",
    "    # Construct operators which are sums of local operators\n",
    "    # (sums of mpos are not always mpos)\n",
    "    H = []\n",
    "    \n",
    "    # Hamiltonian\n",
    "    ## z terms\n",
    "    for i in range(oper_params['L'] - 1 + (oper_params['bc'] == 'c')):\n",
    "        H.append(tensor.mpo(oper_params['L'], d=2))\n",
    "        H[-1][i] = -sz\n",
    "        H[-1][(i+1) % oper_params['L']] = sz\n",
    "    ## x terms\n",
    "    for i in range(oper_params['L']):\n",
    "        H.append(tensor.mpo(oper_params['L'], d=2))\n",
    "        H[-1][i] = -oper_params['h'] * sx\n",
    "        \n",
    "    # Do the MPS\n",
    "    chi_max = 20\n",
    "    rank = tensor.bond_rank(chi_max, oper_params['L'], d=2)\n",
    "    A = tensor.mps(evecs[:, 0], rank, L=oper_params['L'], d=2)\n",
    "    \n",
    "    print(\n",
    "        'L=', oper_params['L'],\n",
    "        'h=', oper_params['h'],\n",
    "        'bc=', oper_params['bc'],\n",
    "    )\n",
    "    print(f'GS energy (ED, N={A.v.size}):', evals[0])\n",
    "    \n",
    "    for i in range(chi_max - 1):\n",
    "        rank = tensor.bond_rank(chi_max - i, oper_params['L'], d=2)\n",
    "        A.lower_rank(rank)\n",
    "        print(\n",
    "            f'GS energy (k={chi_max - i}, N={A.size()}):',\n",
    "            sum(A.expval(e) for e in H) / A.inner(A),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd455d09-610d-4ff3-b41f-e55225fc3762",
   "metadata": {},
   "source": [
    "Wow, that series of operations is really trivial, and the compression ratio is\n",
    "really fantastic!\n",
    "In contrast, what takes a long time is contracting the virtual indices\n",
    "in all $2^L$ possible ways, even when $k=2$ (as it is here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee656f36-7879-4f8b-94a1-9d5e34ef1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time np.inner(A.v, A.contract_bonds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a2c50-15fb-439a-ad2a-f10cd68c2a42",
   "metadata": {},
   "source": [
    "I was wondering why the first program took so long -- clearly because of this\n",
    "exponentially-scaling operation to calculate the overlap, whereas the polynomial\n",
    "tensor network contractions were much faster.\n",
    "The overlap is within 2% at $k=2$, which is amazing, but to actually calculate\n",
    "this on a regular basis would be unfeasible ... unless I write it in Fortran :).\n",
    "For fun, what happens with $k=1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9121e-49f2-456c-9919-d7cd98f2f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = tensor.bond_rank(1, oper_params['L'], d=2)\n",
    "A.lower_rank(rank)\n",
    "%time np.inner(A.v, A.contract_bonds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e97418-a1ed-4127-a0b6-ae97bb2ed181",
   "metadata": {},
   "source": [
    "Not a good approximation anymore!\n",
    "In this case, the storage requirement is just $2L$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be187222-e3a3-4881-97e6-d2b1e26dc67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e78b5-246b-4c77-887a-30e077d36c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
