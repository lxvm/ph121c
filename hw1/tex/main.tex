\documentclass{article}
\linespread{1.075}
\usepackage{times}

\usepackage{physics}

\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    numbers=left,
    frame=single,
}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=red,
}

\usepackage{graphicx}
\graphicspath{ {./include/plots} }

\begin{document}

{\centering

Ph 121c

Assignment 1

Lorenzo Van Munoz

\today

}

\tableofcontents

\newpage

\section{
Dense ED
}

\subsection{
Discussion
}

The Transverse Field Ising Model (TFIM) for a spin chain
of length $L$ with parameter $h$ (and zero indexed) is given by

\begin{align}
    \hat H = - \sum_{j=0}^{L-2} \hat \sigma^z_{j} \hat \sigma^z_{j+1} 
            - \qty( \hat \sigma^z_{L-1}  \hat \sigma^z_{0} )
            - h\sum_{j=0}^{L-1}  \hat \sigma^x_{j}.
\end{align}

I wrote two algorithms for initializing the dense Hamiltonian
corresponding to the TFIM: a direct construction by taking
tensor products (implied in $\hat H$ above) and a direct construction
for any vector by representing the Pauli operators
as bitwise operations on a binary representation of the computational basis.

Since the latter is less prone to memory allocation errors 
(unlike the former, it does not need to repeatedly sum large matrices
and does not use a recursive implementation) I was more comfortable using it.

\subsection{
Results
}

After generating the dense Hamiltonian, I diagonalized it
with the Fortran LAPACK95 routine {\tt syev} to obtain the
spectrum of the Hamiltonian.
For this problem, we are just interested in the ground state
energies as a function of the parameters $L$ and $h$.

\begin{figure}[h]
\input{include/plots/1_dense_ed.tex}
\end{figure}

As a function of $L$, it appears that in figures A1 and A2, larger values of $L$
have lower ground state energies that get increasingly lower as $h$ increases.
This is because in the large $h$ regime, the system favors aligning with the
transverse applied field, and since those terms in the Hamiltonian act on each site,
the more site there are, the lower energy can be achieved.
In addition, the logarithmic scale on the horizontal axis shows a shoulder around 
$h\approx 1$, with the ground state energy approximately constant at $h<1$ and
rapidly decreasing for $h>1$.
And I don't know why they look like low-pass filters -- maybe that is just me.

\begin{figure}[h]
\input{include/plots/1_dense_ed_gap.tex}
\end{figure}

This figure shows how much lower in energy the ground state of the closed system
is compared to that of the open system.
Notably, the energy difference has a sigmoid shape as a function of $h$,
with the inflection point at about the critical value of $h=1$.
Thus, the different boundary conditions of the system have a substantial difference
on the ground state energy in the $h<1$ phase, but this becomes negligible for $h>>1$
due to the interaction with the transverse field.
Also, the energy gap is nearly identical as a function of system size, due to the fact
the main difference is due to the additional term in the Hamiltonian with closed boundaries.

\newpage

\section{
Sparse ED
}

\subsection{
Discussion
}

Using intel MKL, I would have two routines for solving
sparse eigenvalue problems for real matrices:
{\tt ?feast\_scsrev } for finding parts of a spectra, and
{\tt mkl\_sparse\_?\_ev } for finding the extremal eigenvalues.
The former routine requires a 3-array CSR format, and the latter
requires the MKL Sparse BLAS CSR format, which is a 4-array format.
I will probably implement a function to do that later in the course,
because for this set I wrote my own Lanczos routine for finding extremal
eigenvalues, and it works beautifully with impressive speed.
Therefore, I circumvented the use sparse array formats because the algorithm
only needs a function that can multiply a vector by the Hamiltonian.
So I probably shouldn't call this section ``Sparse ED''.

I am comfortably able to push my Lanczos implementation to sizes of $L = 24$
for $m = 32$ eigenvalues, but I won't risk more since each run already takes
a minute at that level.
In some cases I knew I hit a computational limit when I tried to run the program
and experienced a segmentation fault error 
(some of these can be avoided by increasing the stack limit).
In other cases the compiler simply failed to compile due to allocation overflows.

Returning to the question of implementing sparse format at a later date,
I will be faced with two very different implementations that mirror
two very different implementations I made for the dense case.
If I go to tensor product route, I will have to find a way to tensor
product sparse matrices from scratch, in whatever format is needed.
If I go the bitwise route, I will have to find out how to put together
the sparse matrix one row at a time
In both cases, I will have to decide whether I consider using only a
symmetric representation or a full one.
In both cases, I will have to find a way to figure out the size of the
sparse matrix, ideally without ever creating the dense matrix, which
presents a problem with allocating the arrays for the sparse matrix
because I don't know their size apriori (hopefully without having to create 
a new array every time I need to add new elements).
Perhaps a linked list would work well to build the matrix, and then 
convert the linked list into an array.
The only thing I know is that the problem becomes
increasingly sparse in larger dimensions. 

\subsection{
Results
}

The plot next page opens a can of worms: the Lanczos algorithm has
a lot of behaviors going on.
A few I can mention are that it get the largest and smallest
eigenvalues (and some in the middle, where the lines intersect)
accurately.
Generally, the Lanczos spectrum is much flatter than the dense spectrum:
it looks linear whereas the dense spectrum has curves.
Also, the Lanczos spectrum has regions where it plateaus -- in particular
and the ends of the spectrum.
I suspect that in these regions, the algorithm has over-converged: too
many eigenvalues were attracted to the extremes
(this sounds like a queue for some fun music).

\begin{figure}[h]
\input{include/plots/2_sparse_ed_cmp.tex}

\caption{
    Parameters: $L = 8, h = 0.3$, open boundary conditions
}
\end{figure}

I don't have much more to comment, but I'll just go ahead and plot
a lot of data I have at different values of $h$, $L$, and for each boundary
condition.


When I pushed the Lanzos implementation to larger values of $L$, I intentionally
told the solver to compute fewer eigenvalues.
This leads to the following distorted spectra,
which nonetheless will obtain accurate ground states.

\newpage

\section{
Convergence with system size
}

\subsection{
Discussion
}

\subsection{
Results
}

\newpage

\section{
Finding the quantum phase transition
}


\subsection{
Discussion
}

\subsection{
Results
}


\newpage

\section{
Magnetic ordering
}


\subsection{
Discussion
}

\subsection{
Results
}

\newpage

\section{
Using Ising symmetry
}

\subsection{
Discussion
}

To transfer from the $z$ basis, $\{\ket{\uparrow}, \ket{\downarrow} \}$,
to the $x$ basis, $\{\ket{+}, \ket{-}\}$, use this operator
\begin{align}
    \hat T = \frac{1}{\sqrt{2}} \mqty( 1 & 1 \\ 1 & -1 ).
\end{align}
Notably, $T$ is its own inverse. 
Rotating the Hamiltonian does the following:
\begin{align}
    \hat H_z \ket{\psi}_z
        \to& \qty( \prod_j \hat T_j ) \hat H_z \ket{\psi}_z
    \\
    &= \qty( \prod_j \hat T_j ) \hat H_z \qty( \prod_j \hat T_j )
        \qty( \prod_j \hat T_j ) \ket{\psi}_z 
    \\
    &= \hat H_x \ket{\psi}_x.
\end{align}
The form of $H_x$ is as follows:
\begin{align}
    \hat H_x =& \qty( \prod_j \hat T_j ) \hat H_z \qty( \prod_j \hat T_j )
    \\
    =& \qty( \prod_j \hat T_j )
        \qty( -\sum_{j=0}^{L-2} \hat \sigma^z_{j} \hat \sigma^z_{j+1} 
            - \hat \sigma^z_{L-1} \hat \sigma^z_{0}
            -h \sum_{j=0}^{L-1} \hat \sigma^x_{j})
        \qty( \prod_j \hat T_j )
    \\
    =& -\sum_{j=0}^{L-2} \hat T_j \hat \sigma^z_{j} \hat T_j
                        \hat T_{j+1} \hat \sigma^z_{j+1} \hat T_{j+1}
        - \hat T_{L-1} \hat \sigma^z_{L-1} \hat T_{L-1} 
             \hat T_{0}  \hat \sigma^z_{0} \hat T_{0} 
        -h \sum_{j=0}^{L-1}  \hat T_{j} \hat \sigma^x_{j} \hat T_{j}
    \\
    =& -\sum_{j=0}^{L-2} \hat \sigma^x_{j} \hat \sigma^x_{j+1}
        - \hat \sigma^x_{L-1} \hat \sigma^x_{0} 
        -h \sum_{j=0}^{L-1}  \hat \sigma^z_{j}.
\end{align}
This effectively interchanges the role of the sign-flip and spin-flip operators.
(Sorry if the notation is confusing: the Hamiltonian is in the $x$ basis but
the Pauli matrices still have the same matrix elements as in the $z$ basis)

With this form of the Hamiltonian, we are one permutation away from expressing
the disjoint symmetry sectors as a Hamiltonian in block diagonal form, with each
block acting on its own parity sector.
Recall the parity operator 
\begin{align}
    \hat U_z = \prod_j \hat \sigma_{j}^x
\end{align}
which in the x basis is diagonal:
\begin{align}
    \hat U_x = \prod_j \hat \sigma_{j}^z.
\end{align}

Notice how if we zero-index the basis states in the $x$ basis by integers,
the parity operator is the same as the parity of the bit string of the integer
if we take the bit parity of zero to correspond to + eigenstates 
and the bit parity of one to correspond to - eigenstates.
So in order to separate the $x$ basis, we just need an efficient way to flip
bit strings that acts as a permutation of $\hat H_x$ to transform it into block
diagonal form.

Fix $L$ and let $\ket{+_j}, \ket{-_j}$ denote the $j$-th basis element of the
$x$ basis in the + and - parity sectors, respectively 
(Note there are $2^{L-1}$ states in each sector).
Let's zero index these states and find a bit operator map that sends the index
$j$ to the $k$th element of the computational $x$ basis, whose Hamiltonian,
$\hat H_x$, we know.
Note we also need the inverse permutation in order to correctly reassign the
action of terms of the Hamiltonian.

One way to do this is to calculate the diagonal entries of $\hat U_x$, 
and to store an array which in the $j$-th index has the value $k$.
The reverse map is obtained by placing $j$ in the $k$-th entry of the
array which was the diagonal of $\hat U_x$
The diagonal of $\hat U_x$ can be constructed recursively as follows:
let $b_j$ be a bit string of length $2^j$, starting with $b_1 = \qty( 0, 1 )$,
and obtain $b_{j+1}$ by concatentation: $b_{j+1} = \mqty( b_j, -b_j)$, where
a minus sign denotes a bitwise flip operator.

Note that the $k$th entry of $b_j$ is the bit string parity of $k$ (zero indexed).
The first few bit strings are:
\begin{align}
    b_1 =& (0, 1)
    \\
    b_2 =& (0, 1, 1, 0)
    \\
    b_3 =& (0, 1, 1, 0, 1, 0, 0, 1)
\end{align}
which translates to the following diagonal entries of $\hat U_x$:
\begin{align}
    \qty(\hat U_x)^{L=1}_{ii} =& (1, -1)
    \\
    \qty(\hat U_x)^{L=2}_{ii} =& (1, -1, -1, 1)
    \\
    \qty(\hat U_x)^{L=3}_{ii} =& (1, -1, -1, 1, -1, 1, 1, -1).
\end{align}
Thus these indices provide all the bookkeeping necessary to take the symmetry sectors
into a block diagonal form.

If you study this experimentally, perhaps using the {\tt parity\_diag} function
I wrote in the {\tt tfim\_dense} module, one finds that for states in $\mathcal H_+$
symmetry sector, the efficient bit operator that does the bookkeeping from the $j$-th
+ parity state to the $k$-th $x$ basis state is $k = 2*j + \epsilon(j)$ where $\epsilon$
is the bit string parity operator ($\epsilon(j) = 0$ if $j$ has an even number of 1's
and $\epsilon(j) = 1$ if $j$ has an odd number of 1's).
Likewise, for the $j$-th state in the - sector, it corresponds to $k = 2*j + (\epsilon^{-1}(j)$
(where $\epsilon^{-1}(j) = \epsilon(j+1)$).
The inverse operation in both cases is $j = (k - (k \% 2)) / 2$.
In Fortran:
\begin{lstlisting}[language=Fortran]
k = (2*j + poppar(j))           ! from + sector to full basis
k = (2*j + (1 .xor. poppar(j))  ! from - sector to full basis
j = (k - mod(k, 2)) / 2         ! Inverse in both sectors
\end{lstlisting}

\subsection{
Results
}



\newpage

\section{
Conclusion
}

This has been an absurd but instructive experience.
Learning Fortran was going to hurt, but I didn't expect
it to hurt for so long.
I feel like I mainly struggled with how to organize my
code into parts that worked well with each other.
I'll say that Fortran discourages convenience and complexity
but as soon as you know what you want to do, implementation
isn't much harder than in other programming languages.

I also spent lots of time reading the intel and MKL documentation.
It is so dense that you can't diagonalize the manual to get
a wide spectrum of knowledge from it in any short amount of time.
It is an excellent resource, in addition to the examples that come
installed with the oneAPI packages.
Now I know that I could work with it much better in the future
since I'll be less ignorant about where to find help and details.

If I could change other things about this assignment, it would be
going through the exercise of making sparse matrix formats 
(as paralyzing as all the choices that need to be made are)
and possibly changing the "Discussion", "Results" format into
"Approach", "Results", and "Discussion" to create space for the right thoughts

\newpage

\section{
Appendix: Code
}

All of my source code is available by viewing or cloning
\href{https://github.com/lxvm/ph121c.git}{this git repository}.
The following sections are a minimal reference about my computer
and the project.

\subsection{
Computing environment
}

I am using the Intel oneAPI base toolkit (with MKL)
and HPC toolkit (with Fortran), which are freely available
\href{https://software.intel.com/content/www/us/en/develop/
articles/free-intel-software-developer-tools.html}{here}.
Here is information about my computer and software:

\lstinputlisting{./include/versions.txt}

This pdf document was generated by latexmk using \TeX\ Live 2020.

\subsection{
Building this project
}

Please refer to the README's in the repository on how to compile the
code and generate the data presented in this document.

\end{document}
